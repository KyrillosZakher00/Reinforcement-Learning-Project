{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aea834",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!py -m pip install cmake \"gym[atari]\" scipy\n",
    "!py -m pip install gym[atari]\n",
    "!pip install gym[toy_text]\n",
    "!pip install gym[accept-rom-license]\n",
    "!pip install ale-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d049368",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gym\\envs\\registration.py:423: UserWarning: \u001b[33mWARN: Custom namespace `ALE` is being overridden by namespace `ALE`. If you are developing a plugin you shouldn't specify a namespace in `register` calls. The namespace is specified through the entry point package metadata.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "from IPython.display import clear_output\n",
    "from ale_py import ALEInterface\n",
    "import numpy as np\n",
    "import random\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be1eeb",
   "metadata": {},
   "source": [
    "## Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbf77fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_env(name):\n",
    "    \"\"\"\n",
    "    This function is for creating the envirnment\n",
    "    \n",
    "    Input:\n",
    "        name (string) : The environment name\n",
    "    \n",
    "    Output:\n",
    "        env\n",
    "    \"\"\"\n",
    "    env = gym.make(name)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce5f25",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c2a8117",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training(env,alpha=0.1,gamma=0.6,epsilon=0.1):\n",
    "    \"\"\"\n",
    "    \n",
    "    This function is for training the model\n",
    "    \n",
    "    Inputs:\n",
    "        env\n",
    "        alpha (float) : Learning rate --> Hyper parameter\n",
    "        gamma (float) : Discount Factor --> Hyper parameter\n",
    "        epsilon (float) : Exploration-Exploitation Factor --> Hyper parameter\n",
    "    \n",
    "    Output: \n",
    "        Q-table (list)\n",
    "        \n",
    "    \"\"\"\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    # For plotting metrics\n",
    "    all_epochs = []\n",
    "    all_penalties = []\n",
    "\n",
    "    for i in range(1, 100001):\n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Training finished.\\n\")\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc1609c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aea4cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_evaluation(env,q_table):\n",
    "    \"\"\"\n",
    "    \n",
    "    This function is for model evaluation\n",
    "    Input: \n",
    "    \n",
    "        q_table (list): from the training function\n",
    "    \n",
    "    Output\n",
    "    \n",
    "    frames (list of dictionaries) : For animating the result\n",
    "    average_timesteps (float) : For time steps\n",
    "    average_penalties (int) : For penalities\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    frames = [] # for animation\n",
    "    \n",
    "    total_epochs, total_penalties = 0, 0\n",
    "    episodes = 100\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        state = env.reset()\n",
    "        epochs, penalties, reward = 0, 0, 0\n",
    "\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            state, reward, done, info = env.step(action)\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            # Put each rendered frame into dict for animation\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "            epochs += 1\n",
    "\n",
    "        total_penalties += penalties\n",
    "        total_epochs += epochs\n",
    "    average_timesteps = total_epochs / episodes\n",
    "    average_penalties = total_penalties / episodes\n",
    "    print(f\"Results after {episodes} episodes:\")\n",
    "    print(f\"Average timesteps per episode: {average_timesteps}\")\n",
    "    print(f\"Average penalties per episode: {average_penalties}\")\n",
    "    \n",
    "    return frames, average_timesteps , average_penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e62741",
   "metadata": {},
   "source": [
    "## Plotting Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3fb870da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frames(frames):\n",
    "    \"\"\"\n",
    "    This funtion for plotting the frames of the episodes\n",
    "    \n",
    "    Input:\n",
    "        frames (list): list of frames\n",
    "    \n",
    "    \"\"\"\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ae3265",
   "metadata": {},
   "source": [
    "#### 1) Turn this code into a module of functions that can use multiple environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbb5d6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_setup(name_=\"Taxi-v3\",alpha=0.1,gamma=0.6,epsilon=0.1):\n",
    "    \"\"\"\n",
    "    This model is for gathering all functions of setuping the environmen, training and evaluation in one function\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        name (string): the name of environment\n",
    "        alpha (float) : Learning rate --> Hyper parameter\n",
    "        gamma (float) : Discount Factor --> Hyper parameter\n",
    "        epsilon (float) : Exploration-Exploitation Factor --> Hyper parameter\n",
    "        \n",
    "    Output:\n",
    "        \n",
    "        frames (list of dictionaries) : For animating the result\n",
    "        average_timesteps (float) : For time steps\n",
    "        average_penalties (int) : For penalities\n",
    "    \n",
    "    \"\"\"\n",
    "    env_ = set_env(name=name_)\n",
    "    q_table_ = model_training(env=env_,alpha=alpha,gamma=gamma,epsilon=epsilon)\n",
    "    frames_, average_timesteps,average_penalties = model_evaluation(env=env_,q_table=q_table_)\n",
    "    return frames_,average_timesteps,average_penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93eaca29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.89\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# \"Taxi-v3\" Environment\n",
    "frames,average_timesteps,average_penalties = model_setup(name_=\"Taxi-v3\",alpha=0.1,gamma=0.6,epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14d48d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1289\n",
      "State: 410\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe4233a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 11.78\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# \"FrozenLake-v1\" Environment\n",
    "frames,average_timesteps,average_penalties = model_setup(name_=\"FrozenLake-v1\",alpha=0.1,gamma=0.6,epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4cebbf80",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "Timestep: 1178\n",
      "State: 5\n",
      "Action: 1\n",
      "Reward: 0.0\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c56d4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 13.0\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "# \"CliffWalking-v1\" Environment\n",
    "frames,average_timesteps,average_penalties = model_setup(name_=\"CliffWalking-v0\",alpha=0.1,gamma=0.6,epsilon=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45f11de9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  o  o  o  o  o  o  o  o  o  o  o\n",
      "o  C  C  C  C  C  C  C  C  C  C  x\n",
      "\n",
      "\n",
      "Timestep: 1300\n",
      "State: 47\n",
      "Action: 2\n",
      "Reward: -1\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b8915",
   "metadata": {},
   "source": [
    "#### 2) Tune alpha, gamma, and/or epsilon using a decay over episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0a76873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuning_model_training(env,alpha=0.3,gamma=0.4,epsilon=0.7):\n",
    "    \"\"\"\n",
    "    This function is for training the model\n",
    "    \n",
    "    Inputs:\n",
    "        env\n",
    "        alpha (float) : Learning rate --> Hyper parameter\n",
    "        gamma (float) : Discount Factor --> Hyper parameter\n",
    "        epsilon (float) : Exploration-Exploitation Factor --> Hyper parameter\n",
    "    \n",
    "    Output: \n",
    "        Q-table (list)\n",
    "        \n",
    "    \"\"\"\n",
    "    q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    # For plotting metrics\n",
    "    all_epochs = []\n",
    "    all_penalties = []\n",
    "    episodes = 100001\n",
    "    \n",
    "    for i in range(1, episodes): \n",
    "        if i > episodes//4 and i < episodes//2:\n",
    "            alpha, gamma, epsilon=alpha-0.05, gamma-0.05, epsilon-0.05\n",
    "        elif i > episodes//2 and i < episodes//(4/3):\n",
    "            alpha, gamma, epsilon=alpha-0.05, gamma-0.05, epsilon-0.05\n",
    "        elif i > episodes// (4/3):\n",
    "            alpha, gamma, epsilon=alpha-0.05, gamma-0.05, epsilon-0.05\n",
    "            \n",
    "        state = env.reset()\n",
    "\n",
    "        epochs, penalties, reward, = 0, 0, 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                action = np.argmax(q_table[state]) # Exploit learned values\n",
    "\n",
    "            next_state, reward, done, info = env.step(action) \n",
    "\n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            state = next_state\n",
    "            epochs += 1\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode: {i}\")\n",
    "\n",
    "    print(\"Training finished.\\n\")\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa14f8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tuning_setup(name_=\"Taxi-v3\",alpha=0.2,gamma=0.4,epsilon=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    This model is for gathering all functions of setuping the environmen, training and evaluation in one function\n",
    "    \n",
    "    Inputs:\n",
    "    \n",
    "        name (string): the name of environment\n",
    "        alpha (float) : Learning rate --> Hyper parameter\n",
    "        gamma (float) : Discount Factor --> Hyper parameter\n",
    "        epsilon (float) : Exploration-Exploitation Factor --> Hyper parameter\n",
    "        \n",
    "    Output:\n",
    "        \n",
    "        frames (list of dictionaries) : For animating the result\n",
    "        average_timesteps (float) : For time steps\n",
    "        average_penalties (int) : For penalities\n",
    "    \n",
    "    \"\"\"\n",
    "    env_ = set_env(name=name_)\n",
    "    q_table_ = tuning_model_training(env=env_,alpha=alpha,gamma=gamma,epsilon=epsilon)\n",
    "    frames_, average_timesteps,average_penalties = model_evaluation(env=env_,q_table=q_table_)\n",
    "    return frames_,average_timesteps,average_penalties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85fd56c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 200.0\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "frames,average_timesteps,average_penalties = model_tuning_setup(name_=\"Taxi-v3\",alpha=0.3,gamma=0.4,epsilon=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f5a657",
   "metadata": {},
   "source": [
    "We decreased alpha by 0.05 and epsilon by 0.1 and increased gamma by 0.1 on each quarter of episodes and we realized that the time of episodes till completing is increased due to decreasing the learning rate step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c98a590",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04c8c34",
   "metadata": {},
   "source": [
    "#### 3) Implement a grid search to discover the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0d3b4765",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Grid_search(env_name=\"Taxi-v3\",param=None):\n",
    "    \"\"\"\n",
    "    This function is for finding the best parameters \n",
    "    \n",
    "    Inputs:\n",
    "        env_name (string): environment name\n",
    "        param (Dict): {'alpha':None, 'gamma':None, 'epsilon':None}\n",
    "        \n",
    "    Outputs:\n",
    "        best_params (Dict): {'alpha':None,'gamma':None,'epsilon':None,'penalty':None,'time step':None}\n",
    "    \n",
    "    \"\"\"\n",
    "    temp_timestep=10000  # any large number\n",
    "    temp_penalties = 10000 # any large number\n",
    "    parameters = param\n",
    "    for alpha in parameters['alpha']:\n",
    "        for gamma in parameters['gamma']:\n",
    "            for epsilon in parameters['epsilon']:\n",
    "                frames,average_timesteps,average_penalties = model_setup(name_=env_name,alpha=alpha,gamma=gamma,epsilon=epsilon)\n",
    "                if average_penalties <= temp_penalties:\n",
    "                    if average_timesteps <= temp_timestep :\n",
    "                        temp_penalties = average_penalties\n",
    "                        temp_timestep = average_timesteps\n",
    "                        best_params = {'alpha':alpha,'gamma':gamma,'epsilon':epsilon,'penalty':temp_penalties,'time step':temp_timestep}\n",
    "    clear_output(wait=True)\n",
    "    print(f\"Best parameters are: {best_params}\")\n",
    "    return best_params;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f59463fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters are: {'alpha': 0.1, 'gamma': 0.2, 'epsilon': 0.4, 'penalty': 0.0, 'time step': 12.49}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.1,\n",
       " 'gamma': 0.2,\n",
       " 'epsilon': 0.4,\n",
       " 'penalty': 0.0,\n",
       " 'time step': 12.49}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters = {'alpha': [0.3,0.2,0.1],'gamma':[0.3,0.2,0.1],'epsilon':[0.4,0.3,0.2]}\n",
    "best_params = Grid_search(env_name=\"Taxi-v3\",param=parameters)\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd56919f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Training finished.\n",
      "\n",
      "Results after 100 episodes:\n",
      "Average timesteps per episode: 12.59\n",
      "Average penalties per episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "frames,average_timesteps,average_penalties = model_setup(name_=\"Taxi-v3\",alpha=0.1,gamma=0.2,epsilon=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e122f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 1259\n",
      "State: 475\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "print_frames(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80141e62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
